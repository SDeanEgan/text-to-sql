{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Pretrained Models With the WikiSQL Dataset\n",
    "\n",
    "Before Hugging Face models can be compared for a given conditional generation task they must be finetuned using an associated dataset. This notebook serves as a pipeline to this end, for use with the WikiSQL dataset. Various model choices are possible, though some small editing of the `model_info` dictionary is necessary. Other datasets could be used here, but not without their own `format_dataset` setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide your token\n",
    "huggingface_token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21hGe7Mr0gos",
    "outputId": "314c5079-a527-458c-fb20-f274ec69bb80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 11 17:19:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.86                 Driver Version: 551.86         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| 70%   51C    P8             34W /  350W |    1217MiB /  12288MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3052    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      3064    C+G   ...a\\Local\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A      3624    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A      6856    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     10804    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     17924    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     18292    C+G   ...a\\Local\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A     21708    C+G   ...h2txyewy\\InputApp\\TextInputHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)  # Should print True if GPU is available\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IADibkaEs79C"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "login(token=huggingface_token)\n",
    "\n",
    "model_info = {'name': 'facebook/bart-base', 'path': 'finetuned/bart-base-wikisql', 'batch_size': 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XBTV-c1nFiAX",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = model_info['name']\n",
    "path = model_info['path']\n",
    "batch_size = model_info['batch_size']\n",
    "epochs = 5 # just picking this because of testing\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(name)\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartForConditionalGeneration\n"
     ]
    }
   ],
   "source": [
    "print(model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format for WikiSQL\n",
    "\n",
    "To prepare training data for finetuning, input prompts are constructed by simply prepending example questions with 'translate to SQL: '. This serves as a minimum complexity approach for later benchmarking of any prepared models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hM3s5q981Ga6",
    "outputId": "1299c801-ba7f-46ba-a422-e052272ed9bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict class details: \n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'target'],\n",
      "        num_rows: 56355\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'target'],\n",
      "        num_rows: 8421\n",
      "    })\n",
      "}) \n",
      " First formatted example: \n",
      " {'input': 'translate to SQL: Which sum of week that had an attendance larger than 55,767 on September 28, 1986?', 'target': 'SELECT SUM Week FROM table WHERE Attendance > 55,767 AND Date = september 28, 1986'}\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetDict({ 'train': load_dataset('wikisql', split='train'),\n",
    "                            'validation': load_dataset('wikisql', split='validation'),\n",
    "                      })\n",
    "\n",
    "def format_dataset(example):\n",
    "    return {'input': 'translate to SQL: ' + example['question'], 'target': example['sql']['human_readable']}\n",
    "\n",
    "formatted_dataset = dataset.map(format_dataset, remove_columns=dataset['train'].column_names).shuffle(seed=42) # also shuffles!\n",
    "\n",
    "print(\"DatasetDict class details: \\n\", formatted_dataset, \"\\n\", \"First formatted example: \\n\", formatted_dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Scheme Depends Upon the Model Choice\n",
    "\n",
    "These tokenization functions cover several T5 and BART based variants. Minimal additional code should be necessary to expand these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ppkB_UwQH7a6"
   },
   "outputs": [],
   "source": [
    "if model.__class__.__name__ == \"T5ForConditionalGeneration\":\n",
    "    # map with tokenizer to provide tokenized dataset to the Seq2SeqTrainer\n",
    "    def tokenize_function(example_batch):\n",
    "        '''use direct tokenizer call, construct encodings dictionary'''\n",
    "        input_encodings = tokenizer(example_batch['input'], padding='max_length', truncation=True, max_length=64)\n",
    "        target_encodings = tokenizer(example_batch['target'], padding='max_length', truncation=True, max_length=64)\n",
    "    \n",
    "    \n",
    "        encodings = {\n",
    "            'input_ids': input_encodings['input_ids'], \n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': target_encodings['input_ids'],\n",
    "            'decoder_attention_mask': target_encodings['attention_mask']\n",
    "        }\n",
    "    \n",
    "        return encodings\n",
    "    \n",
    "    columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']\n",
    "    \n",
    "elif model.__class__.__name__ == \"BartForConditionalGeneration\":\n",
    "    \n",
    "    def tokenize_function(example_batch):\n",
    "        '''use this for bart'''\n",
    "        input_encodings = tokenizer(example_batch['input'], padding='max_length', truncation=True, max_length=64)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            target_encodings = tokenizer(example_batch['target'], padding='max_length', truncation=True, max_length=64)\n",
    "        \n",
    "        encodings = {\n",
    "            'input_ids': input_encodings['input_ids'], \n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': target_encodings['input_ids']\n",
    "        }\n",
    "        # Remove decoder_attention_mask for BART models\n",
    "        return encodings\n",
    "\n",
    "    columns = ['input_ids', 'attention_mask', 'labels']\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True, remove_columns=formatted_dataset['train'].column_names)\n",
    "\n",
    "tokenized_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SqWbevh5Kelo"
   },
   "outputs": [],
   "source": [
    "# Metric calculation \n",
    "# Exact Match https://huggingface.co/spaces/evaluate-metric/exact_match\n",
    "# ROUGE2 score https://huggingface.co/spaces/evaluate-metric/rouge\n",
    "# BLEU score https://huggingface.co/spaces/evaluate-metric/sacrebleu\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # Decode the predictions and labels\n",
    "    pred_ids[pred_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": exact_match.compute(predictions=pred_str, references=label_str)['exact_match'],\n",
    "        \"rouge2\": rouge.compute(predictions=pred_str, references=label_str)[\"rouge2\"],\n",
    "        \"bleu\": sacrebleu.compute(predictions=pred_str, references=label_str)[\"score\"],\n",
    "    }\n",
    "\n",
    "# arguments for Seq2SeqTrainer\n",
    "trainer_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=path,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    "    #fp16=True, \n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=trainer_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    ")\n",
    "\n",
    "# memory stats for the current device\n",
    "if cuda:\n",
    "    pre_allocated = torch.cuda.memory_allocated(device) / (1024**3)  # GB\n",
    "    pre_reserved = torch.cuda.memory_reserved(device) / (1024**3)  # GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Preliminary Run of Evaluate to Verify the Model Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "c-MSUMJqKrqb",
    "outputId": "3cf2a23e-af5b-45c1-9f74-8b8814dd13b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='264' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 08:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 15.01809310913086,\n",
       " 'eval_model_preparation_time': 0.003,\n",
       " 'eval_exact_match': 0.0,\n",
       " 'eval_rouge2': 0.12279714302204521,\n",
       " 'eval_bleu': 3.7702632841491566,\n",
       " 'eval_runtime': 155.8653,\n",
       " 'eval_samples_per_second': 54.027,\n",
       " 'eval_steps_per_second': 0.847}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qZR_ASsEK5Lp",
    "outputId": "7d3b37cc-29a5-4bfe-ee49-38bdad08f864"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4405' max='4405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4405/4405 32:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.673900</td>\n",
       "      <td>0.096609</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.369909</td>\n",
       "      <td>0.832159</td>\n",
       "      <td>71.611776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.094900</td>\n",
       "      <td>0.087119</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.399121</td>\n",
       "      <td>0.843160</td>\n",
       "      <td>73.626441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.082610</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.412540</td>\n",
       "      <td>0.847493</td>\n",
       "      <td>74.432418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.081649</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.419190</td>\n",
       "      <td>0.850105</td>\n",
       "      <td>74.869931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.080982</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.425365</td>\n",
       "      <td>0.852719</td>\n",
       "      <td>75.240423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Z370-I\\project\\project\\lib\\site-packages\\transformers\\modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train output {'global_step': 4405, 'training_loss': 0.14407037438382897, 'metrics': {'train_runtime': 1947.5326, 'train_samples_per_second': 144.683, 'train_steps_per_second': 2.262, 'total_flos': 1.0738030657536e+16, 'train_loss': 0.14407037438382897, 'epoch': 5.0}, 'additional_memory_allocated': 1.1029548645019531, 'additional_memory_reserved': 6.87109375}\n"
     ]
    }
   ],
   "source": [
    "train_output = trainer.train()\n",
    "train_output = train_output._asdict()\n",
    "\n",
    "# end memory stats for the current device\n",
    "if cuda:\n",
    "    post_allocated = torch.cuda.memory_allocated(device) / (1024**3)  # GB\n",
    "    post_reserved = torch.cuda.memory_reserved(device) / (1024**3)  # GB\n",
    "    \n",
    "    train_output['additional_memory_allocated'] = post_allocated - pre_allocated\n",
    "    train_output['additional_memory_reserved'] = post_reserved - pre_reserved\n",
    "\n",
    "print(\"Train output\", train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model and Its Training Metric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4f_eBTCeTHA",
    "outputId": "6c0ea618-bb69-43b2-a999-24f0f941b30e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# store the model and maybe push to huggingface hub?\n",
    "with open(path + \"/train_output.json\", \"w\") as f:\n",
    "    json.dump(train_output, f, indent=4)\n",
    "\n",
    "trainer.save_model()\n",
    "\n",
    "tokenizer.save_pretrained(path)\n",
    "\n",
    "trainer.create_model_card()\n",
    "\n",
    "#trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-V7bmzeQgqtA"
   },
   "outputs": [],
   "source": [
    "# clean up memory\n",
    "del model\n",
    "if cuda:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOKESPh7i0YCiRexy88VPG1",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "T5-wikiSQL-with-HF-transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03772b2541dd43c5a614829c7155cdc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49e47d1b79cf46a3881843ff210016bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2d910b8525c40879a4a31d805e384ab",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_72ce6fb8cb244ced88abc340f5bf1258",
      "value": 1
     }
    },
    "72ce6fb8cb244ced88abc340f5bf1258": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8a378b1afb7a407f93dacd7b6f10a7a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96f366309e644e0cbe7411f4c49f2c10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2d910b8525c40879a4a31d805e384ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a821ed3f960d44b99aefa77814fa23f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb4cf1ebfc514891a253d9cd6a874ffc",
      "placeholder": "​",
      "style": "IPY_MODEL_03772b2541dd43c5a614829c7155cdc3",
      "value": "100%"
     }
    },
    "c740bd8b679448d9ae4badc67c36400f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96f366309e644e0cbe7411f4c49f2c10",
      "placeholder": "​",
      "style": "IPY_MODEL_8a378b1afb7a407f93dacd7b6f10a7a7",
      "value": " 1/1 [00:00&lt;00:00, 11.34ba/s]"
     }
    },
    "cb4cf1ebfc514891a253d9cd6a874ffc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc7ec75bd9d14f4caa3ce9f87ff7bfbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dab3b41563e2413493b796a22776a6d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a821ed3f960d44b99aefa77814fa23f1",
       "IPY_MODEL_49e47d1b79cf46a3881843ff210016bb",
       "IPY_MODEL_c740bd8b679448d9ae4badc67c36400f"
      ],
      "layout": "IPY_MODEL_cc7ec75bd9d14f4caa3ce9f87ff7bfbb"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
